\chapter{Introduction}

Convolutional neural networks have shown a high performance in a variety of tasks during recent decade. Power of convolutional neural networks (CNNs) is mainly due to their ability to aggregate and generalize local features. It turns out that CNN architecture is extremely good and effective when we deal with \textit{structured data}. In early beginning of CNNs milestone works were VGG-19 \cite{Simonyan_Zisserman_2015} and AlexNet \cite{Krizhevsky_Sutskever_Hinton_2017}, where authors proved that convolutional architectures can be way more computationally effective than we thought before. It has also been shown that learned convolutional filters can reflect a local structure of sample.

Nowadays CNNs are used in a variety of tasks such as classification,  image generation, clustering, dimensionality reduction etc. One of possible applications of CNN is neural image compression. In neural image compression we use a compression power of convolutional neural network, which is a stack of convolutional layers, such as \cite{Krizhevsky_Sutskever_Hinton_2017}.

\chapter{Preliminary and background}

Image compression has a several conventional steps, which are usually employed in neural image compression too. These steps are well described in \cite{JPEG-1992}. JPEG standard was introduced in 1992 and adopted as an image compression standard by Joint Photography Expert Group. JPEG has a module structure, so these modules implementation can be changed drastically, while having same interaction with other modules. This helps to replace an old modules by more advanced and modern ones. For example, Huffman Coding \cite{Huffman-Coding} in Coding stage of JPEG algorithm can be replaced by Arithmetic coding \cite{Arithmetic-Coding}. Many papers employ this module structure, so authors can use already existing ones and only work on one part of the whole compression algorithm to further improve exactly the part they are interested in.

To understand main neural image compression approaches, we need to get familiar with autoencoder architecture first. First time introduced in 2006 \cite{Autoencoder_2006}. In this paper authors propose an unusual method to reduce a dimensionality of data: use neural network build up from two components, encoder and decoder. They significantly overcome efficiency of Principal component analysis \cite{pca} in dimensionality reduction. Dimensionality reduction late has become a closely related to neural image compression. Autoencoder consists of two very specific for its' architecture parts: Encoder and Decoder. In the original paper they have (2-4) hidden layers in the encoder and decoder, but now modern approaches can be much more deep. A main principle of autoencoder is that we use encoder to reduce dimensionality (this results to some kind of data compression, and in other words with the help of encoder we compress data to some compact representation). Afterwards we use Decoder to reconstruct data from intermediate representation to it's original shape. Both encoder and decoder are multilayer neural networks, which makes it extremely flexible and open to all the machine learning progress we have and potentially will have in the future. To train such a model we need to initialize these two networks and propagate the information through encoder and decoder sequentially. Then we need to calculate an error. Decoder output has the same shape as the original sample and usually we define loss as a distance from input sample to the decoder output (such as L1 or L2 distances). Later we use any of existing optimizers perform a backpropagation this loss value and adjust weights.

\chapter{Neural image compression}

Neural image compression usually follows an autoencoder pipeline.

\section{Fully convolutional image compression}

Fully convolutional neural network is a type of neural network without an aggregate layer. Usually such an aggregation operation is a pooling. Pooling layer mechanics is simple: we need some functions that is a permutation invariant, so, it can make a generalization. This function for example can be a min, max, sum or average. These functions are irreversible, so we can not find a reverse function. Since conceptually decoder of an autoencoder is basically a reverse function for an encoder, we need something like this. The compression process must be reversible, otherwise we can not decompress an original image.

Fully convolutional neural networks is well-described in \cite{fcn}. Authors use fully convolutional network for semantic segmentation. Semantic segmentation is a special task in computer vision, where the final proposal is to label each pixel of image according to the object it belongs to. Traditionally this task is solved with clustering algorithms. The FCN (fully convolutional network) authors use is a network with only convolutional elements, this network only has a convolutional layers. Since convolutional layer architecture doesn't require a fixed-size input, it is extremely convenient to use these network with non-fixed size input data.

Each input for convolutional layer is a tensor of size $H \times W \times D$, where $H$ and $W$ are spatial dimensions and $D$ is depth dimension. Each next layer is connected to certain area in the previous layers and the more deep we go through the network the bigger is this area. This area is called a \textit{receptive field}

\chapter{Applications}

\chapter{Conclusion}
