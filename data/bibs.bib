
// ========== Image compression ==========
// ==========       BEGIN       ==========

@article{agustsson_generative_2019,
  title    = {Generative {Adversarial} {Networks} for {Extreme} {Learned} {Image} {Compression}},
  abstract = {We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.},
  author   = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Van Gool, Luc},
  month    = aug,
  year     = {2019},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote   = {Comment: E. Agustsson, M. Tschannen, and F. Mentzer contributed equally to this work. ICCV 2019 camera ready version},
  file     = {Agustsson et al_2019_Generative Adversarial Networks for Extreme Learned Image Compression.pdf:C\:\\Users\\nadol\\Zotero\\storage\\2CXCBXFG\\Agustsson et al_2019_Generative Adversarial Networks for Extreme Learned Image Compression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nadol\\Zotero\\storage\\E7E3SI5K\\1804.html:text/html}
}

@article{zhou_end--end_nodate,
  title    = {End-to-end {Optimized} {Image} {Compression} with {Attention} {Mechanism}},
  abstract = {We present an end-to-end trainable image compression framework for low bit-rate and transparent image compression. Our method is based on variational autoencoder, which consists of a nonlinear encoder transformation, a soft quantizer, a nonlinear decoder transformation and an entropy estimation module. The prior probability of the latent representations is modeled by combining a hyperprior autoencoder and a Pixelcnn++ based context module and they are trained jointly with the transformation autoencoder with attention mechanism. In order to improve the compression performance, a non-local convolution based attention mechanism is designed for allocating bits adaptively. Finally, a novel rate allocation algorithm based on linear optimization is used to assign the bits for each image dynamically, considering the bits constraint of the challenge. Across the experimental results on validation and test sets, the optimized framework can generate the highest PSNR and MS-SSIM for low bit-rate compression competition, and cost the lowest bytes for transparent 40db competition.},
  language = {en},
  author   = {Zhou, Lei and Sun, Zhenhong and Wu, Xiangji and Wu, Junmin},
  keywords = {compression},
  pages    = {4},
  file     = {Zhou et al. - End-to-end Optimized Image Compression with Attent.pdf:C\:\\Users\\nadol\\Zotero\\storage\\LS8JUPXI\\Zhou et al. - End-to-end Optimized Image Compression with Attent.pdf:application/pdf}
}

@article{balle_variational_2018,
  title    = {Variational image compression with a scale hyperprior},
  abstract = {We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.},
  author   = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
  month    = may,
  year     = {2018},
  keywords = {compression, Computer Science - Information Theory, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: accepted as a conference contribution to International Conference on Learning Representations 2018}
}

@article{li_comisr_nodate,
  title    = {{COMISR}: {Compression}-{Informed} {Video} {Super}-{Resolution}},
  abstract = {Most video super-resolution methods focus on restoring high-resolution video frames from low-resolution videos without taking into account compression. However, most videos on the web or mobile devices are compressed, and the compression can be severe when the bandwidth is limited. In this paper, we propose a new compressioninformed video super-resolution model to restore highresolution content without introducing artifacts caused by compression. The proposed model consists of three modules for video super-resolution: bi-directional recurrent warping, detail-preserving ﬂow estimation, and Laplacian enhancement. All these three modules are used to deal with compression properties such as the location of the intra-frames in the input and smoothness in the output frames. For thorough performance evaluation, we conducted extensive experiments on standard datasets with a wide range of compression rates, covering many real video use cases. We showed that our method not only recovers high-resolution content on uncompressed frames from the widely-used benchmark datasets, but also achieves state-of-the-art performance in super-resolving compressed videos based on numerous quantitative metrics. We also evaluated the proposed method by simulating streaming from YouTube to demonstrate its effectiveness and robustness. The source codes and trained models are available at https://github.com/google-research/googleresearch/tree/master/comisr.},
  language = {en},
  author   = {Li, Yinxiao and Jin, Pengchong and Yang, Feng and Liu, Ce and Yang, Ming-Hsuan and Milanfar, Peyman},
  pages    = {10},
  file     = {Li et al. - COMISR Compression-Informed Video Super-Resolutio.pdf:C\:\\Users\\nadol\\Zotero\\storage\\E5B7N8LS\\Li et al. - COMISR Compression-Informed Video Super-Resolutio.pdf:application/pdf}
}


@article{wang_substitutional_2021,
  title    = {Substitutional {Neural} {Image} {Compression}},
  abstract = {We describe Substitutional Neural Image Compression (SNIC), a general approach for enhancing any neural image compression model, that requires no data or additional tuning of the trained model. It boosts compression performance toward a flexible distortion metric and enables bit-rate control using a single model instance. The key idea is to replace the image to be compressed with a substitutional one that outperforms the original one in a desired way. Finding such a substitute is inherently difficult for conventional codecs, yet surprisingly favorable for neural compression models thanks to their fully differentiable structures. With gradients of a particular loss backpropogated to the input, a desired substitute can be efficiently crafted iteratively. We demonstrate the effectiveness of SNIC, when combined with various neural compression models and target metrics, in improving compression quality and performing bit-rate control measured by rate-distortion curves. Empirical results of control precision and generation speed are also discussed.},
  author   = {Wang, Xiao and Jiang, Wei and Wang, Wei and Liu, Shan and Kulis, Brian and Chin, Peter},
  month    = may,
  year     = {2021},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  file     = {arXiv Fulltext PDF:C\:\\Users\\nadol\\Zotero\\storage\\D385N7NM\\Wang et al. - 2021 - Substitutional Neural Image Compression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nadol\\Zotero\\storage\\BBNQJJMR\\2105.html:text/html}
}

@article{lu_progressive_2021,
  title    = {Progressive {Neural} {Image} {Compression} with {Nested} {Quantization} and {Latent} {Ordering}},
  abstract = {We present PLONQ, a progressive neural image compression scheme which pushes the boundary of variable bitrate compression by allowing quality scalable coding with a single bitstream. In contrast to existing learned variable bitrate solutions which produce separate bitstreams for each quality, it enables easier rate-control and requires less storage. Leveraging the latent scaling based variable bitrate solution, we introduce nested quantization, a method that defines multiple quantization levels with nested quantization grids, and progressively refines all latents from the coarsest to the finest quantization level. To achieve finer progressiveness in between any two quantization levels, latent elements are incrementally refined with an importance ordering defined in the rate-distortion sense. To the best of our knowledge, PLONQ is the first learning-based progressive image coding scheme and it outperforms SPIHT, a well-known wavelet-based progressive image codec.},
  journal  = {arXiv:2102.02913 [cs]},
  author   = {Lu, Yadong and Zhu, Yinhao and Yang, Yang and Said, Amir and Cohen, Taco S.},
  month    = feb,
  year     = {2021},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  file     = {arXiv Fulltext PDF:C\:\\Users\\nadol\\Zotero\\storage\\RISEWSWG\\Lu et al. - 2021 - Progressive Neural Image Compression with Nested Q.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\nadol\\Zotero\\storage\\EKTEFSR5\\2102.html:text/html}
}

@misc{noauthor_view_nodate,
  title    = {View synthesis-based light field image compression using a generative adversarial network {\textbar} {Elsevier} {Enhanced} {Reader}},
  url      = {https://reader.elsevier.com/reader/sd/pii/S0020025520307532?token=15F3E6BC7991BF1C37F7621B41FA02B4742CD5FE0B11C956F30BA4E6059428DA177B780F52871599B28FFFA79F042F97&originRegion=eu-west-1&originCreation=20211021092417},
  language = {en},
  doi      = {10.1016/j.ins.2020.07.073},
  file     = {Snapshot:C\:\\Users\\nadol\\Zotero\\storage\\MMK8RV3K\\S0020025520307532.html:text/html;View synthesis-based light field image compression.pdf:C\:\\Users\\nadol\\Zotero\\storage\\6UCGTB68\\View synthesis-based light field image compression.pdf:application/pdf}
}

@article{liu_unified_2020,
  title    = {A {Unified} {End}-to-{End} {Framework} for {Efficient} {Deep} {Image} {Compression}},
  abstract = {Image compression is a widely used technique to reduce the spatial redundancy in images. Recently, learning based image compression has achieved significant progress by using the powerful representation ability from neural networks. However, the current state-of-the-art learning based image compression methods suffer from the huge computational cost, which limits their capacity for practical applications. In this paper, we propose a unified framework called Efficient Deep Image Compression (EDIC) based on three new technologies, including a channel attention module, a Gaussian mixture model and a decoder-side enhancement module. Specifically, we design an auto-encoder style network for learning based image compression. To improve the coding efficiency, we exploit the channel relationship between latent representations by using the channel attention module. Besides, the Gaussian mixture model is introduced for the entropy model and improves the accuracy for bitrate estimation. Furthermore, we introduce the decoder-side enhancement module to further improve image compression performance. Our EDIC method can also be readily incorporated with the Deep Video Compression (DVC) framework to further improve the video compression performance. Simultaneously, our EDIC method boosts the coding performance significantly while bringing slightly increased computational cost. More importantly, experimental results demonstrate that the proposed approach outperforms the current state-of-the-art image compression methods and is up to more than 150 times faster in terms of decoding speed when compared with Minnen's method. The proposed framework also successfully improves the performance of the recent deep video compression system DVC. Our code will be released at https://github.com/liujiaheng/compression.},
  author   = {Liu, Jiaheng and Lu, Guo and Hu, Zhihao and Xu, Dong},
  month    = may,
  year     = {2020},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: We will released our code and training data}
}

@article{mentzer_practical_2020,
  title    = {Practical {Full} {Resolution} {Learned} {Lossless} {Image} {Compression}},
  abstract = {We propose the first practical learned lossless image compression system, L3C, and show that it outperforms the popular engineered codecs, PNG, WebP and JPEG 2000. At the core of our method is a fully parallelizable hierarchical probabilistic model for adaptive entropy coding which is optimized end-to-end for the compression task. In contrast to recent autoregressive discrete probabilistic models such as PixelCNN, our method i) models the image distribution jointly with learned auxiliary representations instead of exclusively modeling the image distribution in RGB space, and ii) only requires three forward-passes to predict all pixel probabilities instead of one for each pixel. As a result, L3C obtains over two orders of magnitude speedups when sampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN). Furthermore, we find that learning the auxiliary representation is crucial and outperforms predefined auxiliary representations such as an RGB pyramid significantly.},
  author   = {Mentzer, Fabian and Agustsson, Eirikur and Tschannen, Michael and Timofte, Radu and Van Gool, Luc},
  month    = mar,
  year     = {2020},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: Updated preprocessing and Table 1, see A.1 in supplementary. Code and models: https://github.com/fab-jul/L3C-PyTorch}
}

@article{mentzer_high-fidelity_2020,
  title    = {High-{Fidelity} {Generative} {Image} {Compression}},
  abstract = {We extensively study how to combine Generative Adversarial Networks and learned compression to obtain a state-of-the-art generative lossy compression system. In particular, we investigate normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses. In contrast to previous work, i) we obtain visually pleasing reconstructions that are perceptually similar to the input, ii) we operate in a broad range of bitrates, and iii) our approach can be applied to high-resolution images. We bridge the gap between rate-distortion-perception theory and practice by evaluating our approach both quantitatively with various perceptual metrics, and with a user study. The study shows that our method is preferred to previous approaches even if they use more than 2x the bitrate.},
  author   = {Mentzer, Fabian and Toderici, George and Tschannen, Michael and Agustsson, Eirikur},
  month    = oct,
  year     = {2020},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: This is the Camera Ready version for NeurIPS 2020. Project page: https://hific.github.io}
}

@article{cao_lossless_2020,
  title    = {Lossless {Image} {Compression} through {Super}-{Resolution}},
  abstract = {We introduce a simple and efficient lossless image compression algorithm. We store a low resolution version of an image as raw pixels, followed by several iterations of lossless super-resolution. For lossless super-resolution, we predict the probability of a high-resolution image, conditioned on the low-resolution input, and use entropy coding to compress this super-resolution operator. Super-Resolution based Compression (SReC) is able to achieve state-of-the-art compression rates with practical runtimes on large datasets. Code is available online at https://github.com/caoscott/SReC.},
  author   = {Cao, Sheng and Wu, Chao-Yuan and Krähenbühl, Philipp},
  month    = apr,
  year     = {2020},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: Tech report}
}

@article{ronneberger_u-net_2015,
  title      = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
  shorttitle = {U-{Net}},
  abstract   = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  month      = may,
  year       = {2015},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: conditionally accepted at MICCAI 2015}
}

@article{dumoulin_adversarially_2017,
  title    = {Adversarially {Learned} {Inference}},
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  author   = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  month    = feb,
  year     = {2017},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{donahue_adversarial_2017,
  title    = {Adversarial {Feature} {Learning}},
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  author   = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
  month    = apr,
  year     = {2017},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  annote   = {Comment: Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2 results improved 1-2\% due to averaging predictions over 10 crops at test time, as done in Noroozi \& Favaro; Table 3 VOC classification results slightly improved due to minor bugfix. (See v6 changelog for previous versions.)}
}

@article{salimans_improved_2016,
  title    = {Improved {Techniques} for {Training} {GANs}},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  author   = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  month    = jun,
  year     = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{radford_unsupervised_2016,
  title    = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  author   = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  month    = jan,
  year     = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  annote   = {Comment: Under review as a conference paper at ICLR 2016}
}

@article{sugawara_checkerboard_2019,
  title    = {Checkerboard artifacts free convolutional neural networks},
  volume   = {8},
  doi      = {10.1017/ATSIP.2019.2},
  abstract = {It is well-known that a number of convolutional neural networks (CNNs) generate checkerboard artifacts in both of two processes: forward-propagation of upsampling layers and backpropagation of convolutional layers. A condition for avoiding the artifacts is proposed in this paper. So far, these artifacts have been studied mainly for linear multirate systems, but the conventional condition for avoiding them cannot be applied to CNNs due to the non-linearity of CNNs. We extend the avoidance condition for CNNs and apply the proposed structure to typical CNNs to confirm whether the novel structure is effective. Experimental results demonstrate that the proposed structure can perfectly avoid generating checkerboard artifacts while keeping the excellent properties that CNNs have.},
  journal  = {APSIPA Transactions on Signal and Information Processing},
  author   = {Sugawara, Yusuke and Shiota, Sayaka and Kiya, Hitoshi},
  year     = {2019},
  keywords = {enhance}
}

@article{odena_deconvolution_2016,
  title    = {Deconvolution and {Checkerboard} {Artifacts}},
  volume   = {1},
  issn     = {2476-0757},
  url      = {http://distill.pub/2016/deconv-checkerboard},
  doi      = {10.23915/distill.00003},
  abstract = {When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.},
  language = {en},
  number   = {10},
  journal  = {Distill},
  author   = {Odena, Augustus and Dumoulin, Vincent and Olah, Chris},
  month    = oct,
  year     = {2016},
  keywords = {enhance},
  pages    = {e3}
}

@article{shi_real-time_2016,
  title    = {Real-{Time} {Single} {Image} and {Video} {Super}-{Resolution} {Using} an {Efficient} {Sub}-{Pixel} {Convolutional} {Neural} {Network}},
  abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
  author   = {Shi, Wenzhe and Caballero, Jose and Huszár, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  month    = sep,
  year     = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
  annote   = {Comment: CVPR 2016 paper with updated affiliations and supplemental material, fixed typo in equation 4}
}

@article{toderici_full_2017,
  title    = {Full {Resolution} {Image} {Compression} with {Recurrent} {Neural} {Networks}},
  abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3\%-8.8\% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
  author   = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  month    = jul,
  year     = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote   = {Comment: Updated with content for CVPR and removed supplemental material to an external link for size limitations}
}

@article{li_learning_2017,
  title    = {Learning {Convolutional} {Networks} for {Content}-weighted {Image} {Compression}},
  abstract = {Lossy image compression is generally formulated as a joint rate-distortion optimization to learn encoder, quantizer, and decoder. However, the quantizer is non-differentiable, and discrete entropy estimation usually is required for rate control. These make it very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that the bit rate of the different parts of the image should be adapted to local content. And the content aware bit rate is allocated under the guidance of a content-weighted importance map. Thus, the sum of the importance map can serve as a continuous alternative of discrete entropy estimation to control compression rate. And binarizer is adopted to quantize the output of encoder due to the binarization scheme is also directly defined by the importance map. Furthermore, a proxy function is introduced for binary operation in backward propagation to make it differentiable. Therefore, the encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner by using a subset of the ImageNet database. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.},
  author   = {Li, Mu and Zuo, Wangmeng and Gu, Shuhang and Zhao, Debin and Zhang, David},
  month    = sep,
  year     = {2017},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition}
}

@article{balle_end--end_2017,
  title    = {End-to-end {Optimized} {Image} {Compression}},
  abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
  author   = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
  month    = mar,
  year     = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory},
  annote   = {Comment: Published as a conference paper at ICLR 2017}
}

@article{theis_lossy_2017,
  title    = {Lossy {Image} {Compression} with {Compressive} {Autoencoders}},
  abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
  author   = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
  month    = mar,
  year     = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning}
}

@article{liu_deep_nodate,
  title    = {Deep {Image} {Compression} via {End}-to-{End} {Learning}},
  abstract = {We present a lossy image compression method based on deep convolutional neural networks (CNNs), which outperforms the existing BPG, WebP, JPEG2000 and JPEG as measured via multi-scale structural similarity (MS-SSIM), at the same bit rate. Currently, most of the CNNs based approaches train the network using a l-2 loss between the reconstructions and the ground-truths in the pixel domain, which leads to over-smoothing results and visual quality degradation especially at a very low bit rate. Therefore, we improve the subjective quality with the combination of a perception loss and an adversarial loss additionally. To achieve better rate-distortion optimization (RDO), we also introduce an easy-to-hard transfer learning when adding quantization error and rate constraint. Finally, we evaluate our method on public Kodak and the Test Dataset P/M released by the Computer Vision Lab of ETH Zurich, resulting in averaged 7.81\% and 19.1\% BD-rate reduction over BPG, respectively.},
  language = {en},
  author   = {Liu, Haojie and Chen, Tong and Shen, Qiu and Yue, Tao and Ma, Zhan},
  pages    = {4},
  file     = {Liu et al. - Deep Image Compression via End-to-End Learning.pdf:C\:\\Users\\nadol\\Zotero\\storage\\89GMID4I\\Liu et al. - Deep Image Compression via End-to-End Learning.pdf:application/pdf}
}

@article{tellez_neural_2021,
  title    = {Neural {Image} {Compression} for {Gigapixel} {Histopathology} {Image} {Analysis}},
  volume   = {43},
  issn     = {0162-8828, 2160-9292, 1939-3539},
  doi      = {10.1109/TPAMI.2019.2936841},
  abstract = {We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.},
  number   = {2},
  urldate  = {2022-03-25},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author   = {Tellez, David and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
  month    = feb,
  year     = {2021},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  pages    = {567--578},
  annote   = {Comment: Accepted in the IEEE Transactions on Pattern Analysis and Machine Intelligence journal}
}

@article{matsubara_sc2_2022,
  title      = {{SC2}: {Supervised} {Compression} for {Split} {Computing}},
  shorttitle = {{SC2}},
  abstract   = {Split computing distributes the execution of a neural network (e.g., for a classification task) between a mobile device and a more powerful edge server. A simple alternative to splitting the network is to carry out the supervised task purely on the edge server while compressing and transmitting the full data, and most approaches have barely outperformed this baseline. This paper proposes a new approach for discretizing and entropy-coding intermediate feature activations to efficiently transmit them from the mobile device to the edge server. We show that a efficient splittable network architecture results from a three-way tradeoff between (a) minimizing the computation on the mobile device, (b) minimizing the size of the data to be transmitted, and (c) maximizing the model's prediction performance. We propose an architecture based on this tradeoff and train the splittable network and entropy model in a knowledge distillation framework. In an extensive set of experiments involving three vision tasks, three datasets, nine baselines, and more than 180 trained models, we show that our approach improves supervised rate-distortion tradeoffs while maintaining a considerably smaller encoder size. We also release sc2bench, an installable Python package, to encourage and facilitate future studies on supervised compression for split computing (SC2).},
  author     = {Matsubara, Yoshitomo and Yang, Ruihan and Levorato, Marco and Mandt, Stephan},
  month      = mar,
  year       = {2022},
  keywords   = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  annote     = {Comment: Preprint. Code and models are available at https://github.com/yoshitomo-matsubara/sc2-benchmark}
}

@article{li_learning_2020,
  title    = {Learning a {Single} {Model} with a {Wide} {Range} of {Quality} {Factors} for {JPEG} {Image} {Artifacts} {Removal}},
  volume   = {29},
  issn     = {1057-7149, 1941-0042},
  doi      = {10.1109/TIP.2020.3020389},
  abstract = {Lossy compression brings artifacts into the compressed image and degrades the visual quality. In recent years, many compression artifacts removal methods based on convolutional neural network (CNN) have been developed with great success. However, these methods usually train a model based on one specific value or a small range of quality factors. Obviously, if the test image's quality factor does not match to the assumed value range, then degraded performance will be resulted. With this motivation and further consideration of practical usage, a highly robust compression artifacts removal network is proposed in this paper. Our proposed network is a single model approach that can be trained for handling a wide range of quality factors while consistently delivering superior or comparable image artifacts removal performance. To demonstrate, we focus on the JPEG compression with quality factors, ranging from 1 to 60. Note that a turnkey success of our proposed network lies in the novel utilization of the quantization tables as part of the training data. Furthermore, it has two branches in parallel---i.e., the restoration branch and the global branch. The former effectively removes the local artifacts, such as ringing artifacts removal. On the other hand, the latter extracts the global features of the entire image that provides highly instrumental image quality improvement, especially effective on dealing with the global artifacts, such as blocking, color shifting. Extensive experimental results performed on color and grayscale images have clearly demonstrated the effectiveness and efficacy of our proposed single-model approach on the removal of compression artifacts from the decoded image.},
  urldate  = {2022-04-05},
  journal  = {IEEE Transactions on Image Processing},
  author   = {Li, Jianwei and Wang, Yongtao and Xie, Haihua and Ma, Kai-Kuang},
  year     = {2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, enhance},
  pages    = {8842--8854},
  annote   = {Comment: Accepted for publication in the IEEE Transactions on Image Processing}
}

@article{wallace_jpeg_1992,
  title    = {The {JPEG} still picture compression standard},
  volume   = {38},
  issn     = {1558-4127},
  doi      = {10.1109/30.125072},
  abstract = {A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method.{\textless}{\textgreater}},
  number   = {1},
  journal  = {IEEE Transactions on Consumer Electronics},
  author   = {Wallace, G.K.},
  year     = {1992},
  note     = {Conference Name: IEEE Transactions on Consumer Electronics},
  keywords = {Costs, Digital images, Displays, Facsimile, Gray-scale, Image coding, Image storage, ISO standards, Standards development, Transform coding, compression},
  pages    = {xviii--xxxiv}
}

@article{ehrlich_quantization_2020,
  title    = {Quantization {Guided} {JPEG} {Artifact} {Correction}},
  abstract = {The JPEG image compression algorithm is the most popular method of image compression because of its ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current state-of-the-art methods require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG files quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings.},
  author   = {Ehrlich, Max and Davis, Larry and Lim, Ser-Nam and Shrivastava, Abhinav},
  month    = jul,
  year     = {2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, enhance, Statistics - Machine Learning},
  annote   = {Comment: Published in the proceedings of ECCV 2020, please see our released code and models at https://gitlab.com/Queuecumber/quantization-guided-ac}
}

@article{kamisli_end--end_2022,
  title    = {End-to-{End} {Learned} {Block}-{Based} {Image} {Compression} with {Block}-{Level} {Masked} {Convolutions} and {Asymptotic} {Closed} {Loop} {Training}},
  abstract = {Learned image compression research has achieved state-of-the-art compression performance with auto-encoder based neural network architectures, where the image is mapped via convolutional neural networks (CNN) into a latent representation that is quantized and processed again with CNN to obtain the reconstructed image. CNN operate on entire input images. On the other hand, traditional state-of-the-art image and video compression methods process images with a block-by-block processing approach for various reasons. Very recently, work on learned image compression with block based approaches have also appeared, which use the auto-encoder architecture on large blocks of the input image and introduce additional neural networks that perform intra/spatial prediction and deblocking/post-processing functions. This paper explores an alternative learned block-based image compression approach in which neither an explicit intra prediction neural network nor an explicit deblocking neural network is used. A single auto-encoder neural network with block-level masked convolutions is used and the block size is much smaller (8x8). By using block-level masked convolutions, each block is processed using reconstructed neighboring left and upper blocks both at the encoder and decoder. Hence, the mutual information between adjacent blocks is exploited during compression and each block is reconstructed using neighboring blocks, resolving the need for explicit intra prediction and deblocking neural networks. Since the explored system is a closed loop system, a special optimization procedure, the asymptotic closed loop design, is used with standard stochastic gradient descent based training. The experimental results indicate competitive image compression performance.},
  author   = {Kamisli, Fatih},
  month    = mar,
  year     = {2022},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing}
}

@article{xie_enhanced_2021,
  title    = {Enhanced {Invertible} {Encoding} for {Learned} {Image} {Compression}},
  abstract = {Although deep learning based image compression methods have achieved promising progress these days, the performance of these methods still cannot match the latest compression standard Versatile Video Coding (VVC). Most of the recent developments focus on designing a more accurate and flexible entropy model that can better parameterize the distributions of the latent features. However, few efforts are devoted to structuring a better transformation between the image space and the latent feature space. In this paper, instead of employing previous autoencoder style networks to build this transformation, we propose an enhanced Invertible Encoding Network with invertible neural networks (INNs) to largely mitigate the information loss problem for better compression. Experimental results on the Kodak, CLIC, and Tecnick datasets show that our method outperforms the existing learned image compression methods and compression standards, including VVC (VTM 12.1), especially for high-resolution images. Our source code is available at https://github.com/xyq7/InvCompress.},
  author   = {Xie, Yueqi and Cheng, Ka Leong and Chen, Qifeng},
  month    = aug,
  year     = {2021},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: Accepted to ACM Multimedia 2021 as Oral}
}

@article{bai_towards_2021,
  title    = {Towards {End}-to-{End} {Image} {Compression} and {Analysis} with {Transformers}},
  abstract = {We propose an end-to-end image compression and analysis model with Transformers, targeting to the cloud-based image classification application. Instead of placing an existing Transformer-based image classification model directly after an image codec, we aim to redesign the Vision Transformer (ViT) model to perform image classification from the compressed features and facilitate image compression with the long-term information from the Transformer. Specifically, we first replace the patchify stem (i.e., image splitting and embedding) of the ViT model with a lightweight image encoder modelled by a convolutional neural network. The compressed features generated by the image encoder are injected convolutional inductive bias and are fed to the Transformer for image classification bypassing image reconstruction. Meanwhile, we propose a feature aggregation module to fuse the compressed features with the selected intermediate features of the Transformer, and feed the aggregated features to a deconvolutional neural network for image reconstruction. The aggregated features can obtain the long-term information from the self-attention mechanism of the Transformer and improve the compression performance. The rate-distortion-accuracy optimization problem is finally solved by a two-step training strategy. Experimental results demonstrate the effectiveness of the proposed model in both the image compression and the classification tasks.},
  author   = {Bai, Yuanchao and Yang, Xu and Liu, Xianming and Jiang, Junjun and Wang, Yaowei and Ji, Xiangyang and Gao, Wen},
  year     = {2021},
  keywords = {compression, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: Accepted by AAAI 2022; Code: https://github.com/BYchao100/Towards-Image-Compression-and-Analysis-with-Transformers}
}

@article{jiang_towards_2021,
  title    = {Towards {Flexible} {Blind} {JPEG} {Artifacts} {Removal}},
  abstract = {Training a single deep blind model to handle different quality factors for JPEG image artifacts removal has been attracting considerable attention due to its convenience for practical usage. However, existing deep blind methods usually directly reconstruct the image without predicting the quality factor, thus lacking the flexibility to control the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convolutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off between artifacts removal and details preservation. Specifically, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the predicted quality factor into the subsequent reconstructor module through a quality factor attention block for flexible control. Besides, we find existing methods are prone to fail on non-aligned double JPEG images even with only a one-pixel shift, and we thus propose a double JPEG degradation model to augment the training data. Extensive experiments on single JPEG images, more general double JPEG images, and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative metrics and visual quality.},
  author   = {Jiang, Jiaxi and Zhang, Kai and Timofte, Radu},
  month    = sep,
  year     = {2021},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, 94A08, I.2, I.4, enhance},
  annote   = {Comment: Accepted by ICCV 2021, Code: https://github.com/jiaxi-jiang/FBCNN}
}

@misc{noauthor_clic_nodate,
  title = {{CLIC} · {Challenge} on {Learned} {Image} {Compression}},
  url   = {http://compression.cc/},
  file  = {CLIC · Challenge on Learned Image Compression:C\:\\Users\\nadol\\Zotero\\storage\\RB4DJCRX\\compression.cc.html:text/html}
}

@techreport{kingma_adam_2017,
  title       = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle  = {Adam},
  abstract    = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  institution = {arXiv},
  author      = {Kingma, Diederik P. and Ba, Jimmy},
  year        = {2017},
  keywords    = {Computer Science - Machine Learning},
  annote      = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015}
}

@article{zou_devil_2022,
  title      = {The {Devil} {Is} in the {Details}: {Window}-based {Attention} for {Image} {Compression}},
  shorttitle = {The {Devil} {Is} in the {Details}},
  abstract   = {Learned image compression methods have exhibited superior rate-distortion performance than classical image compression standards. Most existing learned image compression models are based on Convolutional Neural Networks (CNNs). Despite great contributions, a main drawback of CNN based model is that its structure is not designed for capturing local redundancy, especially the non-repetitive textures, which severely affects the reconstruction quality. Therefore, how to make full use of both global structure and local texture becomes the core problem for learning-based image compression. Inspired by recent progresses of Vision Transformer (ViT) and Swin Transformer, we found that combining the local-aware attention mechanism with the global-related feature learning could meet the expectation in image compression. In this paper, we first extensively study the effects of multiple kinds of attention mechanisms for local features learning, then introduce a more straightforward yet effective window-based local attention block. The proposed window-based attention is very flexible which could work as a plug-and-play component to enhance CNN and Transformer models. Moreover, we propose a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in the down-sampling encoder and up-sampling decoder. Extensive experimental evaluations have shown that the proposed method is effective and outperforms the state-of-the-art methods. The code is publicly available at https://github.com/Googolxx/STF.},
  author     = {Zou, Renjie and Song, Chunfeng and Zhang, Zhaoxiang},
  month      = mar,
  year       = {2022},
  keywords   = {compression, Computer Science - Computer Vision and Pattern Recognition},
  annote     = {Comment: Accepted by CVPR 2022}
}

// ========== Image compression ==========
// ==========        END        ==========

@article{fcn,
  title    = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  author   = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  month    = mar,
  year     = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote   = {Comment: to appear in CVPR (2015)}
}

@article{toderici_full_2017,
  title    = {Full {Resolution} {Image} {Compression} with {Recurrent} {Neural} {Networks}},
  abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3\%-8.8\% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
  author   = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  month    = jul,
  year     = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annote   = {Comment: Updated with content for CVPR and removed supplemental material to an external link for size limitations}
}

 @inproceedings{zhang2017learning,
  title     = {Learning Deep CNN Denoiser Prior for Image Restoration},
  author    = {Zhang, Kai and Zuo, Wangmeng and Gu, Shuhang and Zhang, Lei},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {3929--3938},
  year      = {2017}
}
@article{zhang2020plug,
  title  = {Plug-and-Play Image Restoration with Deep Denoiser Prior},
  author = {Zhang, Kai and Li, Yawei and Zuo, Wangmeng and Zhang, Lei and Van Gool, Luc and Timofte, Radu},
  year   = {2020}
}
@article{Autoencoder_2006,
  author  = {Geoffrey Hinton and Ruslan Salakhutdinov},
  title   = {Reducing the Dimensionality of Data with Neural 
             Networks},
  journal = {Science},
  volume  = {313},
  number  = {5786},
  pages   = {504 - 507},
  year    = {2006}
}

@misc{autoencoder_papers,
  title    = {Papers with {Code} - {AutoEncoder} {Explained}},
  url      = {https://paperswithcode.com/method/autoencoder},
  abstract = {An Autoencoder is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder). Image: Michael Massi},
  language = {en}
}

@article{mentzer_high_fidelity_2020,
  title    = {High-{Fidelity} {Generative} {Image} {Compression}},
  abstract = {We extensively study how to combine Generative Adversarial Networks and learned compression to obtain a state-of-the-art generative lossy compression system. In particular, we investigate normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses. In contrast to previous work, i) we obtain visually pleasing reconstructions that are perceptually similar to the input, ii) we operate in a broad range of bitrates, and iii) our approach can be applied to high-resolution images. We bridge the gap between rate-distortion-perception theory and practice by evaluating our approach both quantitatively with various perceptual metrics, and with a user study. The study shows that our method is preferred to previous approaches even if they use more than 2x the bitrate.},
  author   = {Mentzer, Fabian and Toderici, George and Tschannen, Michael and Agustsson, Eirikur},
  month    = oct,
  year     = {2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  annote   = {Comment: This is the Camera Ready version for NeurIPS 2020. Project page: https://hific.github.io}
}

@article{JPEG-1992,
  title   = {The JPEG still picture compression standard},
  volume  = {38},
  number  = {1},
  journal = {IEEE Transactions on Consumer Electronics},
  author  = {Wallace, G.K.},
  year    = {1992},
  note    = {Conference Name: IEEE Transactions on Consumer Electronics},
  pages   = {xviii--xxxiv}
}

@article{Huffman-Coding,
  author  = {Huffman, David A.},
  journal = {Proceedings of the IRE},
  title   = {A Method for the Construction of Minimum-Redundancy Codes},
  year    = {1952},
  volume  = {40},
  number  = {9},
  pages   = {1098-1101},
  doi     = {10.1109/JRPROC.1952.273898}
}

@article{Arithmetic-Coding,
  author     = {Witten, Ian H. and Neal, Radford M. and Cleary, John G.},
  title      = {Arithmetic Coding for Data Compression},
  year       = {1987},
  issue_date = {June 1987},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {30},
  number     = {6},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/214762.214771},
  doi        = {10.1145/214762.214771},
  abstract   = {The state of the art in data compression is arithmetic coding, not the better-known Huffman method. Arithmetic coding gives greater compression, is faster for adaptive models, and clearly separates the model from the channel encoding.},
  journal    = {Commun. ACM},
  month      = {jun},
  pages      = {520–540},
  numpages   = {21}
}

@article{pca,
  author    = { Karl   Pearson   F.R.S. },
  title     = {LIII. On lines and planes of closest fit to systems of points in space},
  journal   = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume    = {2},
  number    = {11},
  pages     = {559-572},
  year      = {1901},
  publisher = {Taylor & Francis},
  doi       = {10.1080/14786440109462720}
}
 @article{Ballé_Minnen_Singh_Hwang_Johnston_2018,
  title        = {Variational image compression with a scale hyperprior},
  abstractnote = {We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.},
  author       = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
  year         = {2018},
  month        = {May}
}

 @article{Zhao_Meng_Yin_Sigal_2019,
  title        = {Image Generation from Layout},
  abstractnote = {Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse generation. The proposed Layout2Im model significantly outperforms the previous state of the art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our method’s ability to generate complex and diverse images with multiple objects.},
  author       = {Zhao, Bo and Meng, Lili and Yin, Weidong and Sigal, Leonid},
  year         = {2019},
  month        = {Oct}
}

 @article{Balle_Laparra_Simoncelli_2017,
  title        = {End-to-end Optimized Image Compression},
  abstractnote = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
  author       = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
  year         = {2017},
  month        = {Mar}
}
 @article{Toderici_Vincent_Johnston_Hwang_Minnen_Shor_Covell_2017,
  title        = {Full Resolution Image Compression with Recurrent Neural Networks},
  abstractnote = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study “one-shot” versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
  author       = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  year         = {2017},
  month        = {Jul}
}
 @article{Wu_Wan_Yue_Jin_Zhao_Golmant_Gholaminejad_Gonzalez_Keutzer_2017,
  title        = {Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions},
  abstractnote = {Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. In this paper, we present a parameter-free, FLOP-free “shift” operation as an alternative to spatial convolutions. We fuse shifts and point-wise convolutions to construct end-to-end trainable shift-based modules, with a hyperparameter characterizing the tradeoff between accuracy and efficiency. To demonstrate the operation’s efficacy, we replace ResNet’s 3x3 convolutions with shift-based modules for improved CIFAR10 and CIFAR100 accuracy using 60% fewer parameters; we additionally demonstrate the operation’s resilience to parameter reduction on ImageNet, outperforming ResNet family members. We finally show the shift operation’s applicability across domains, achieving strong performance with fewer parameters on classification, face verification and style transfer.},
  author       = {Wu, Bichen and Wan, Alvin and Yue, Xiangyu and Jin, Peter and Zhao, Sicheng and Golmant, Noah and Gholaminejad, Amir and Gonzalez, Joseph and Keutzer, Kurt},
  year         = {2017},
  month        = {Dec}
}
 @article{Goodfellow_Pouget-Abadie_Mirza_Xu_Warde-Farley_Ozair_Courville_Bengio_2014,
  title        = {Generative Adversarial Networks},
  abstractnote = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  author       = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year         = {2014},
  month        = {Jun}
}
@misc{kodak,
  title = {True {Color} {Kodak} {Images}},
  fil   = {http://r0k.us/graphics/kodak/},
  urle  = {True Color Kodak Images:C\:\\Users\\nadol\\Zotero\\storage\\SQ4V2P25\\kodak.html:text/html}
}

@article{OpenImages2,
  title   = {OpenImages: A public dataset for large-scale multi-label and multi-class image classification.},
  author  = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Kamali, Shahab and Malloci, Matteo and Pont-Tuset, Jordi and Veit, Andreas and Belongie, Serge and Gomes, Victor and Gupta, Abhinav and Sun, Chen and Chechik, Gal and Cai, David and Feng, Zheyun and Narayanan, Dhyanesh and Murphy, Kevin},
  journal = {Dataset available from https://storage.googleapis.com/openimages/web/index.html},
  year    = {2017}
}
 @inbook{Wang_Yu_Wu_Gu_Liu_Dong_Qiao_Loy_2019,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks},
  volume       = {11133},
  isbn         = {978-3-030-11020-8},
  url          = {http://link.springer.com/10.1007/978-3-030-11021-5_5},
  doi          = {10.1007/978-3-030-11021-5_5},
  abstractnote = {The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Beneﬁting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the ﬁrst place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.},
  booktitle    = {Computer Vision – ECCV 2018 Workshops},
  publisher    = {Springer International Publishing},
  author       = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Loy, Chen Change},
  editor       = {Leal-Taixé, Laura and Roth, Stefan},
  year         = {2019},
  pages        = {63–79},
  collection   = {Lecture Notes in Computer Science}
}

@article{schonfeld_u-net_2021,
  title    = {A {U}-{Net} {Based} {Discriminator} for {Generative} {Adversarial} {Networks}},
  abstract = {Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals dataset. The code is available at https://github.com/boschresearch/unetgan.},
  author   = {Schönfeld, Edgar and Schiele, Bernt and Khoreva, Anna},
  month    = mar,
  year     = {2021},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, gan},
  annote   = {Comment: CVPR 2020 (Main Conference). Code repository: https://github.com/boschresearch/unetgan}
}

 @article{Wang_Xie_Dong_Shan_2021,
  title        = {Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},
  abstractnote = {Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
  author       = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  year         = {2021},
  month        = {Aug}
}
 @article{Ledig_Theis_Huszar_Caballero_Cunningham_Acosta_Aitken_Tejani_Totz_Wang_et_al_2017,
  title        = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
  abstractnote = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  author       = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year         = {2017},
  month        = {May}
}
